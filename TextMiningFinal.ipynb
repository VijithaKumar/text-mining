{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Libraries used in the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline,TransformerMixin,FeatureUnion\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from nltk.corpus import stopwords, treebank\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read input file\n",
    "df = pandas.read_csv(\"/Users/Vijitha/Desktop/Spring/IDS 566/wikipedia.annotated_train-2.csv\")\n",
    "\n",
    "#Read positive and Negative Dictionaries\n",
    "pos=pandas.read_csv(\"/Users/Vijitha/Desktop/Spring/IDS 566/Positive words.csv\")\n",
    "positive=pos[\"Words\"]\n",
    "neg=pandas.read_csv(\"/Users/Vijitha/Desktop/Spring/IDS 566/Negative words.csv\")\n",
    "negative=neg[\"Words\"]\n",
    "\n",
    "#Read Dependent variable\n",
    "Y=df['politeness'].as_matrix()\n",
    "\n",
    "#Read Independent Variable\n",
    "X=df['Request']\n",
    "\n",
    "# Word list for Hedges politeness strategy\n",
    "hedges = [\n",
    "    \"think\", \"thought\", \"thinking\", \"almost\",\n",
    "    \"apparent\", \"apparently\", \"appear\", \"appeared\", \"appears\", \"approximately\", \"around\",\n",
    "    \"assume\", \"assumed\", \"certain amount\", \"certain extent\", \"certain level\", \"claim\",\n",
    "    \"claimed\", \"doubt\", \"doubtful\", \"essentially\", \"estimate\",\n",
    "    \"estimated\", \"feel\", \"felt\", \"frequently\", \"from our perspective\", \"generally\", \"guess\",\n",
    "    \"in general\", \"in most cases\", \"in most instances\", \"in our view\", \"indicate\", \"indicated\",\n",
    "    \"largely\", \"likely\", \"mainly\", \"may\", \"maybe\", \"might\", \"mostly\", \"often\", \"on the whole\",\n",
    "    \"ought\", \"perhaps\", \"plausible\", \"plausibly\", \"possible\", \"possibly\", \"postulate\",\n",
    "    \"postulated\", \"presumable\", \"probable\", \"probably\", \"relatively\", \"roughly\", \"seems\",\n",
    "    \"should\", \"sometimes\", \"somewhat\", \"suggest\", \"suggested\", \"suppose\", \"suspect\", \"tend to\",\n",
    "    \"tends to\", \"typical\", \"typically\", \"uncertain\", \"uncertainly\", \"unclear\", \"unclearly\",\n",
    "    \"unlikely\", \"usually\", \"broadly\", \"tended to\", \"presumably\", \"suggests\",\n",
    "    \"from this perspective\", \"from my perspective\", \"in my view\", \"in this view\", \"in our opinion\",\n",
    "    \"in my opinion\", \"to my knowledge\", \"fairly\", \"quite\", \"rather\", \"argue\", \"argues\", \"argued\",\n",
    "    \"claims\", \"feels\", \"indicates\", \"supposed\", \"supposes\", \"suspects\", \"postulates\"\n",
    "]\n",
    "\n",
    "#Read the acronym_expansion file for wikipedia\n",
    "acro = pandas.read_csv(r\"/Users/Vijitha/Desktop/Spring/IDS 566/Text acronym.csv\")\n",
    "text=numpy.asarray(acro[\"T\"])\n",
    "actual=numpy.asarray(acro[\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaner function\n",
    "def cleaner(s):\n",
    "    ss=re.sub(r'[^a-zA-Z]', ' ', s) #keeps alpha\n",
    "\n",
    "    tokens = nltk.word_tokenize(ss)\n",
    "    tokens_lower=map(lambda x:x.lower(),tokens) #convert all words to lower\n",
    "    replaced = []\n",
    "    for item in tokens_lower:\n",
    "        words = item\n",
    "        for i, j in enumerate(text):\n",
    "            if item == j:\n",
    "                words = item.replace(item, actual[i])\n",
    "        replaced.append(words)\n",
    "    filtered_words = [word for word in replaced if word not in stopwords.words('english')] #remove all stop words\n",
    "    remove_url = [word for word in filtered_words if not \"url\" in word] #remove 'url'\n",
    "    return remove_url\n",
    "\n",
    "#TFIDF on unigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1),token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "#QMarks count\n",
    "class QMarks(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.count('?')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#QMarks atStart\n",
    "class StartQ(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.startswith('?')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Normalize values\n",
    "class Norm(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return DataFrame(preprocessing.normalize(X))\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Strategy: Hedges\n",
    "class Hedgeser(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: len(set(nltk.word_tokenize(l.lower())).intersection(hedges))))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Positive lexicon\n",
    "class Poser(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: len(set(nltk.word_tokenize(l.lower())).intersection(set(positive)))))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Negative Lexicon\n",
    "class Neger(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: len(set(nltk.word_tokenize(l.lower())).intersection(set(negative)))))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    #Strategy: Counterfactual Modals\n",
    "class SubjunctiveTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: \"could you\" in s.lower() or \"would you\" in s.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Indicative Modal\n",
    "class IndicativeTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: \"can you\" in s.lower() or \"will you\" in s.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Gratitude\n",
    "class Gratitude(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: \"i appreciate\" in s.lower() or \"thank\" in s.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Deference\n",
    "class Deference(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"great\",\"good\",\"nice\",\"good\",\"interesting\",\"cool\",\"excellent\",\"awesome\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Greeting\n",
    "class Greeting(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"hi\",\"hello\",\"hey\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: DirectStart\n",
    "class DirectStart(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"so\",\"then\",\"and\",\"but\",\"or\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: First person start\n",
    "class FirstpStart(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"i\",\"my\",\"mine\",\"myself\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: First person plural\n",
    "class FirstpPlural(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: len(set(nltk.word_tokenize(l.lower())).intersection([\"we\", \"our\", \"us\", \"ourselves\"]))>0))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Second Person\n",
    "class SecondPerson(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: (len(set(nltk.word_tokenize(l.lower())).intersection([\"you\",\"your\",\"yours\",\"yourself\"]))>0)and (l.partition(' ')[0].lower() not in [\"you\",\"your\",\"yours\",\"yourself\"])))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Second person start\n",
    "class SecondpStart(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"you\",\"your\",\"yours\",\"yourself\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: PleaseStart\n",
    "class PleaseStart(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"please\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: FirstPerson\n",
    "class FirstPerson(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: (len(set(nltk.word_tokenize(l.lower())).intersection([\"i\", \"my\", \"mine\", \"myself\"]))>0)and (l.partition(' ')[0].lower() not in [\"i\", \"my\", \"mine\", \"myself\"])))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Please\n",
    "class Please(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: (len(set(nltk.word_tokenize(l.lower())).intersection([\"please\"]))>0)and (l.partition(' ')[0].lower() not in [\"please\"])))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Factuality\n",
    "class Factuality(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda l: (len(set(nltk.word_tokenize(l.lower())).intersection([\"really\", \"actually\", \"honestly\", \"surely\"]))>0) or \"the point\" in l.lower() or \"the reality\" in l.lower() or \"the truth\" in l.lower() or \"in fact\" in l.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Direct Question\n",
    "class Question(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: s.partition(' ')[0].lower() in [\"please\"]))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Indirect (btw)\n",
    "class Bytheway(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: \"by the way\" in s.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#Strategy: Apologizing\n",
    "class Apologizing(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: len(set(nltk.word_tokenize(s.lower())).intersection([\"sorry\", \"whoops\",\"oops\",\"excuse\", \"regret\", \"admit\",\"plea\",]))>0 or \"i apologize\" in s.lower() or \"forgive me\" in s.lower() or \"excuse me\" in s.lower()))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#POSTags: Modal\n",
    "class POSTaggerMD(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: sum(1 for x in dict(nltk.pos_tag(s)).values() if x=='MD')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#POSTags: Pronouns\n",
    "class POSTaggerPRP(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: sum(1 for x in dict(nltk.pos_tag(s)).values() if x=='PRP' or x=='PRP$')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#POSTags: Adverbs\n",
    "class POSTaggerWD(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: sum(1 for x in dict(nltk.pos_tag(s)).values() if x=='RB' or x=='RBR' or x=='RBS')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#POSTags: Adjectives\n",
    "class POSTaggerJJ(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: sum(1 for x in dict(nltk.pos_tag(s)).values() if x=='JJ' or x=='JJR' or x=='JJS')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#POSTags: Past verbs\n",
    "class POSTaggerVB(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        modals = DataFrame(X.apply(lambda s: sum(1 for x in dict(nltk.pos_tag(s)).values() if x=='VBD' or x=='VBN')))\n",
    "        return modals\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic 0:\n",
      "article think see would could hi person like thanks articles also one reason anything removed nowiki point work wondering perhaps going well know category even wikipedia really else agree name added give without tell please change deleted ask template project question add made non explain feel find noticed part make\n",
      "Topic 1:\n",
      "ok would edit one know like article think link make right reply good get way could say wikipedia edits help need rather much see hey better mean yes thanks people something time saw list find account problem sure go sources said still also come two use place maybe found name\n",
      "Topic 2:\n",
      "page talk article could would please look user image wikipedia take new discussion want time something one like reply put use hi made think help comment pages may editing articles delete get since images deleted still back know deletion block maybe add sure someone admin read might move question need\n"
     ]
    }
   ],
   "source": [
    "#Unsupervised Topic Modeling\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, tokenizer=cleaner)\n",
    "tf = tf_vectorizer.fit_transform(X)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 3\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "display_topics(lda, tf_feature_names, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generic Model Fit if we want to use multiple models in FeatureUnion\n",
    "class ModelTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return DataFrame(self.model.predict(X))\n",
    "\n",
    "#Combine features to run in parallel\n",
    "combined_features= FeatureUnion([(\"tfd\",vectorizer),\n",
    "                                (\"pos\",SubjunctiveTransformer()),\n",
    "                                (\"pos1\",IndicativeTransformer()),\n",
    "                                (\"try\", Pipeline([\n",
    "                                    ('modal',POSTaggerMD()),\n",
    "                                    ('scale', Norm())\n",
    "                                    ])),\n",
    "                                (\"try1\", Pipeline([\n",
    "                                    ('pro',POSTaggerPRP()),\n",
    "                                    ('scale', Norm())\n",
    "                                    ])),\n",
    "                                (\"try2\", Pipeline([\n",
    "                                    ('wd',POSTaggerWD()),\n",
    "                                    ('scale', Norm())\n",
    "                                    ])),\n",
    "                                (\"try3\", Pipeline([\n",
    "                                    ('adj',POSTaggerJJ()),\n",
    "                                    ('scale', Norm())\n",
    "                                    ])),\n",
    "                                (\"try4\", Pipeline([\n",
    "                                    ('verb',POSTaggerVB()),\n",
    "                                    ('scale', Norm())\n",
    "                                    ])),\n",
    "                                (\"hedges\", Pipeline([\n",
    "                                    ('hed',Hedgeser()),\n",
    "                                    ('scale',Norm())\n",
    "                                    ])),\n",
    "                                (\"positive\",Pipeline([\n",
    "                                    ('posi',Poser()),\n",
    "                                    ('scale',Norm())\n",
    "                                    ])),\n",
    "                                (\"negative\",Pipeline([\n",
    "                                    ('nega',Neger()),\n",
    "                                    ('scale',Norm())\n",
    "                                    ])),\n",
    "                                (\"qmarks\",Pipeline([\n",
    "                                    ('qm',QMarks()),\n",
    "                                    ('scale',Norm())\n",
    "                                    ])),\n",
    "                                (\"gratitude\",Gratitude()),\n",
    "                                (\"startq\",StartQ()),\n",
    "                                (\"deference\",Deference()),\n",
    "                                (\"greeting\", Greeting()),\n",
    "                                (\"dstart\", DirectStart()),\n",
    "                                (\"firststart\", FirstpStart()),\n",
    "                                (\"firstplu\", FirstpPlural()),\n",
    "                                (\"secper\", SecondPerson()),\n",
    "                                (\"secpstart\", SecondpStart()),\n",
    "                                (\"pls\", PleaseStart()),\n",
    "                                (\"fp\", FirstPerson()),\n",
    "                                (\"please\", Please()),\n",
    "                                (\"fact\", Factuality()),\n",
    "                                (\"qs\", Question()),\n",
    "                                (\"btw\", Bytheway()),\n",
    "                                (\"sorry\", Apologizing())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features generated\n"
     ]
    }
   ],
   "source": [
    "#Fit training data\n",
    "X_features = combined_features.fit(X, Y).transform(X)\n",
    "X_features.toarray()\n",
    "\n",
    "#Read the test data\n",
    "dftest = pandas.read_csv(\"/Users/Vijitha/Desktop/Spring/IDS 566/wikipedia.annotated_test.csv\")\n",
    "\n",
    "Y1=dftest['Label'].as_matrix()\n",
    "\n",
    "X1=dftest['Request']\n",
    "\n",
    "#Transform test data\n",
    "Test_features=combined_features.transform(X1)\n",
    "print \"features generated\"\n",
    "\n",
    "#Final Model\n",
    "clf = svm.SVC(kernel='linear', C = 1.0)\n",
    "clf.fit(X_features.toarray(),Y)\n",
    "\n",
    "#Cross validate on Test and Train\n",
    "pred=cross_val_predict(clf, Test_features.toarray(), Y1, cv=3)\n",
    "predtr = cross_val_predict(clf,X_features.toarray(),Y,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   impolite       0.59      0.34      0.43       922\n",
      "    neutral       0.60      0.80      0.69      2053\n",
      "     polite       0.67      0.47      0.55      1025\n",
      "\n",
      "avg / total       0.61      0.61      0.59      4000\n",
      "\n",
      "[[ 311  587   24]\n",
      " [ 187 1652  214]\n",
      " [  31  515  479]]\n",
      "score\n",
      "0.6105\n",
      "test\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   impolite       0.44      0.26      0.33        80\n",
      "    neutral       0.59      0.82      0.68       183\n",
      "     polite       0.64      0.36      0.46        90\n",
      "\n",
      "avg / total       0.57      0.58      0.55       353\n",
      "\n",
      "[[ 21  53   6]\n",
      " [ 21 150  12]\n",
      " [  6  52  32]]\n",
      "score\n",
      "0.57507082153\n"
     ]
    }
   ],
   "source": [
    "#Print Metrics of Classifier\n",
    "print \"train\"\n",
    "print(metrics.classification_report(Y, predtr))\n",
    "print(metrics.confusion_matrix(Y,predtr))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y, predtr)\n",
    "\n",
    "print \"test\"\n",
    "print(metrics.classification_report(Y1, pred))\n",
    "print(metrics.confusion_matrix(Y1,pred))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM implementation to find best parameters\n",
    "svm = SVC()\n",
    "\n",
    "# Do grid search over kernel, degree and C:\n",
    "\n",
    "pipeline = Pipeline([(\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(svm__kernel=[\"linear\",\"poly\",\"rbf\"],\n",
    "                  svm__degree=[2,3],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X_features.toarray(), Y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Naive Bayes Implementation\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_features.toarray(), Y)\n",
    "\n",
    "pred = cross_val_predict(clf, Test_features.toarray(), Y1, cv=5)\n",
    "predtr = cross_val_predict(clf, X_features.toarray(), Y, cv=5)\n",
    "print \"GaussianNB\"\n",
    "print \"train\"\n",
    "print(metrics.classification_report(Y, predtr))\n",
    "print(metrics.confusion_matrix(Y, predtr))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y, predtr)\n",
    "print \"GaussianNB\"\n",
    "print \"test\"\n",
    "print(metrics.classification_report(Y1, pred))\n",
    "print(metrics.confusion_matrix(Y1, pred))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y1, pred)\n",
    "\n",
    "clf = CalibratedClassifierCV(clf, method='sigmoid')\n",
    "clf.fit(X_features.toarray(), Y)\n",
    "\n",
    "pred = cross_val_predict(clf, Test_features.toarray(), Y1, cv=5)\n",
    "predtr = cross_val_predict(clf, X_features.toarray(), Y, cv=5)\n",
    "print \"Sigmoid\"\n",
    "print \"train\"\n",
    "print(metrics.classification_report(Y, predtr))\n",
    "print(metrics.confusion_matrix(Y, predtr))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y, predtr)\n",
    "print \"Sigmoid\"\n",
    "print \"test\"\n",
    "print(metrics.classification_report(Y1, pred))\n",
    "print(metrics.confusion_matrix(Y1, pred))\n",
    "print \"score\"\n",
    "print metrics.accuracy_score(Y1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#knn implementation for various values of k \n",
    "#iterate for different values of k\n",
    "for k in [12,20,18,3,30,25,7]:\n",
    "    clf = KNeighborsClassifier(k,weights='distance')\n",
    "    clf.fit(X_features.toarray(),Y)\n",
    "\n",
    "    pred=cross_val_predict(clf, Test_features.toarray(), Y1, cv=3)\n",
    "    predtr = cross_val_predict(clf,X_features.toarray(),Y,cv=3)\n",
    "    print k\n",
    "    print \"train\"\n",
    "    print(metrics.classification_report(Y, predtr))\n",
    "    print(metrics.confusion_matrix(Y,predtr))\n",
    "    print \"score\"\n",
    "    print metrics.accuracy_score(Y, predtr)\n",
    "    print k\n",
    "    print \"test\"\n",
    "    print(metrics.classification_report(Y1, pred))\n",
    "    print(metrics.confusion_matrix(Y1,pred))\n",
    "    print \"score\"\n",
    "    print metrics.accuracy_score(Y1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logit implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DT implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
